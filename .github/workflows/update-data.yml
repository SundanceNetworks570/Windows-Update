name: Update MSRC data JSON (daily)

on:
  schedule:
    - cron: "9 9 * * *"   # every day at 09:09 UTC
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (stdlib only today)
        run: echo "No extra deps required"

      - name: Build data.json (RSS; CVE/KB enrich; known-issues)
        run: |
          set -euo pipefail

          python - << 'PY'
          import json, re, sys, time, datetime, urllib.request, xml.etree.ElementTree as ET

          # ---- Settings
          RSS_URL = "https://api.msrc.microsoft.com/update-guide/rss"
          DAYS_BACK = 30

          # ---- Helpers
          KB_RE     = re.compile(r"\bKB(\d{5,8})\b", re.IGNORECASE)
          KB_URL_RE = re.compile(r"/kb/(\d{5,8})", re.IGNORECASE)
          CVE_RE    = re.compile(r"\bCVE-\d{4}-\d{4,7}\b", re.IGNORECASE)

          def uniq(seq):
            seen=set(); out=[]
            for x in seq:
              if not x: continue
              if x not in seen:
                seen.add(x); out.append(x)
            return out

          def known_issues_url_for_kb(kb):
            # Synthesize a Release Health search (consistent & reliable)
            return f"https://learn.microsoft.com/search/?terms={kb}%20known%20issues%20windows%20release%20health"

          def fetch(url, timeout=30):
            req = urllib.request.Request(url, headers={"User-Agent": "MSRC-Updates/1.0"})
            with urllib.request.urlopen(req, timeout=timeout) as r:
              return r.read()

          # ---- Get RSS
          try:
            rss_bytes = fetch(RSS_URL)
          except Exception as e:
            print("ERROR: unable to fetch SUG RSS:", e)
            sys.exit(0)

          # ---- Parse RSS (Atom-ish)
          # Feed contains <item> with <title>, <link>, <description>, <pubDate>
          root = ET.fromstring(rss_bytes)
          channel = root.find("channel")
          items = []
          if channel is not None:
            for it in channel.findall("item"):
              title = (it.findtext("title") or "").strip()
              link  = (it.findtext("link") or "").strip()
              desc  = (it.findtext("description") or "").strip()
              pub   = (it.findtext("pubDate") or "").strip()
              items.append({"title": title, "link": link, "description": desc, "pubDate": pub})
          else:
            # some variants nest differentlyâ€”fallback
            for it in root.findall(".//item"):
              title = (it.findtext("title") or "").strip()
              link  = (it.findtext("link") or "").strip()
              desc  = (it.findtext("description") or "").strip()
              pub   = (it.findtext("pubDate") or "").strip()
              items.append({"title": title, "link": link, "description": desc, "pubDate": pub})

          # ---- Time filter (last 30 days)
          def within_window(pub):
            try:
              # Example: "Mon, 04 Nov 2025 00:00:00 GMT"
              dt = datetime.datetime.strptime(pub, "%a, %d %b %Y %H:%M:%S %Z")
            except:
              return True  # keep if unknown
            cutoff = datetime.datetime.utcnow() - datetime.timedelta(days=DAYS_BACK)
            return dt.replace(tzinfo=None) >= cutoff

          items = [x for x in items if within_window(x.get("pubDate",""))]

          # ---- Enrich CVEs, KBs, known-issues url, and keep back-compat fields
          enriched = []
          for it in items:
            title = it.get("title","")
            desc  = it.get("description","")
            link  = it.get("link","")
            text  = f"{title} {desc} {link}"

            cves = uniq([m.upper() for m in CVE_RE.findall(text)])
            kbs  = uniq([("KB"+m).upper() for m in KB_RE.findall(text)] +
                        [("KB"+m).upper() for m in KB_URL_RE.findall(text)])

            known_url = ""
            if kbs:
              known_url = known_issues_url_for_kb(kbs[0])

            out = {
              "title": title,
              "description": desc or "(no description)",
              "link": link,
              "releaseDate": it.get("pubDate",""),
              "source": "SUG RSS",
              # canonical fields
              "cves": cves,
              "kbs": kbs,
              "knownIssuesUrl": known_url,
              # back-compat for your page & any older code
              "cve": ", ".join(cves) if cves else "",
              "kb":  ", ".join(kbs)  if kbs  else "",
              "knownIssues": known_url or "",
            }
            enriched.append(out)

          # ---- Sort newest first (by pubDate string)
          enriched.sort(key=lambda x: x.get("releaseDate",""), reverse=True)

          # ---- Write data.json (only if changed)
          import pathlib, json, hashlib
          p = pathlib.Path("data.json")
          new = json.dumps(enriched, ensure_ascii=False, indent=2)
          old = p.read_text(encoding="utf-8") if p.exists() else ""
          if new != old:
            p.write_text(new, encoding="utf-8")
            print(f"Wrote {len(enriched)} entries to data.json")
          else:
            print("data.json unchanged")

          PY

      - name: Commit and push if changed
        run: |
          set -e
          if ! git diff --quiet -- data.json; then
            git config user.name  "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add data.json
            git commit -m "MSRC: refresh data.json (RSS, CVE/KB enrich, known-issues link)"
            git push
          else
            echo "No changes to commit."
