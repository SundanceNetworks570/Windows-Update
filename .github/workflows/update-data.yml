name: Build MSRC updates JSON (daily)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 9 * * *"   # 09:00 UTC daily

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests python-dateutil

      - name: Build data.json (SUG v2 global API; robust JSON handling)
        id: build
        run: |
          python - <<'PY'
          import json, time, random, sys, os
          from datetime import datetime, timedelta, timezone
          import requests

          # ---- time window (today back 30 days) ----
          NOW = datetime.now(timezone.utc)
          START = NOW - timedelta(days=30)

          # slice the window to reduce throttling (6 chunks √ó 5 days)
          def make_slices(start, end, span_days=5):
            cur = start
            while cur < end:
              nxt = min(cur + timedelta(days=span_days), end)
              yield cur, nxt
              cur = nxt

          # Strong headers to avoid HTTP 999 (anti-bot)
          HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
            "Accept": "application/json",
            "Accept-Language": "en-US,en;q=0.9",
            "Origin": "https://msrc.microsoft.com",
            "Referer": "https://msrc.microsoft.com/update-guide",
            "Connection": "keep-alive",
          }

          BASE = "https://api.msrc.microsoft.com/sug/v2.0/updates"

          # We pull only fields we need to keep payload small
          SELECT = "title,cveIds,releaseDate,lastModifiedDate"
          ORDER  = "lastModifiedDate%20desc"

          def fetch_chunk(since_iso, until_iso, max_tries=12):
            # filter between [since, until)
            filtr = f"lastModifiedDate%20ge%20{since_iso}%20and%20lastModifiedDate%20lt%20{until_iso}"
            url = f"{BASE}?$select={SELECT}&$filter={filtr}&$orderby={ORDER}&$top=500"
            backoff = 15
            tries = 0
            while tries < max_tries:
              tries += 1
              try:
                r = requests.get(url, headers=HEADERS, timeout=30)
              except Exception as e:
                print(f"‚ö†Ô∏è  Request error ({tries}/{max_tries}): {e}")
                time.sleep(backoff + random.uniform(0, 4))
                backoff = min(backoff * 2, 300)
                continue

              ct = r.headers.get("content-type","").lower()
              if r.status_code == 200 and "json" in ct:
                try:
                  data = r.json().get("value", [])
                  return data
                except Exception as e:
                  print(f"‚ö†Ô∏è  JSON parse error ({tries}/{max_tries}): {e}")

              # MSRC returns 999 when it rate-limits automation
              if r.status_code in (999, 429) or r.status_code >= 500 or "json" not in ct:
                print(f"‚è≥ MSRC throttled or non-JSON (HTTP {r.status_code}); retrying in {backoff}s‚Ä¶ ({tries}/{max_tries})")
                time.sleep(backoff + random.uniform(0, 4))
                backoff = min(backoff * 2, 300)
                continue

              # 4xx other than 429/999: nothing more to do for this chunk
              print(f"‚ö†Ô∏è  HTTP {r.status_code} ‚Äî giving up this chunk.")
              return []

            print("‚ùå All retries failed for chunk.")
            return []

          def iso(dt):
            # SUG expects Zulu ISO 8601
            return dt.strftime("%Y-%m-%dT%H:%M:%SZ")

          all_items = []
          for s, e in make_slices(START, NOW, span_days=5):
            print(f"üîé Fetching {iso(s)} .. {iso(e)}")
            items = fetch_chunk(iso(s), iso(e))
            print(f"  ‚Üí {len(items)} items")
            all_items.extend(items)

          # Deduplicate by CVE + title + lastModifiedDate
          seen = set()
          uniq = []
          for it in all_items:
            key = (
              tuple(it.get("cveIds") or []),
              it.get("title",""),
              it.get("lastModifiedDate",""),
            )
            if key not in seen:
              seen.add(key)
              uniq.append(it)

          # Keep only last 30 days strictly (safety)
          def to_dt(s):
            try:
              return datetime.fromisoformat(s.replace("Z","+00:00"))
            except:
              return None

          filtered = []
          for it in uniq:
            dt = to_dt(it.get("lastModifiedDate","")) or to_dt(it.get("releaseDate",""))
            if dt and START <= dt.replace(tzinfo=timezone.utc) <= NOW:
              filtered.append(it)

          out = sorted(
            filtered,
            key=lambda x: x.get("lastModifiedDate", x.get("releaseDate","")),
            reverse=True
          )

          print(f"‚úÖ Total kept (last 30 days): {len(out)}")

          # Write only if we actually have valid entries; otherwise keep existing
          if out:
            with open("data.json", "w", encoding="utf-8") as f:
              json.dump(out, f, ensure_ascii=False, indent=2)
            print(f"üíæ Wrote {len(out)} rows to data.json (window {START.date()}..{NOW.date()})")
          else:
            if os.path.exists("data.json"):
              print("‚ÑπÔ∏è  No valid entries found ‚Äî preserving existing data.json.")
            else:
              # create empty but valid file the first time
              with open("data.json", "w", encoding="utf-8") as f:
                json.dump([], f)
              print("‚ÑπÔ∏è  Created empty data.json (no results).")

          PY

      - name: Commit if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): daily MSRC data.json update"
          file_pattern: data.json
