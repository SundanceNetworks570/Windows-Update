name: Build MSRC updates JSON (daily)

on:
  # Manual trigger button in Actions
  workflow_dispatch:
  # Runs every day at 9:00 UTC (change if you want)
  schedule:
    - cron: "0 9 * * *"

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install requests

      - name: Build data.json (SUG v2 global API; robust JSON handling)
        run: |
          python - << 'PY'
          import json, os, time, requests
          from datetime import datetime, timedelta, timezone

          # ---- Config ----
          DAYS_BACK = 30
          MAX_RETRIES = 5
          RETRY_SLEEP = 15  # seconds between retries on rate limit (HTTP 999)

          # SUG v2 API; select only the fields we need and order by most recent
          URL = ("https://api.msrc.microsoft.com/sug/v2.0/updates"
                 "?$select=title,cveIds,releaseDate,lastModifiedDate,currentReleaseDate"
                 "&$orderby=lastModifiedDate%20desc&$top=500")

          NOW = datetime.now(timezone.utc)
          SINCE = NOW - timedelta(days=DAYS_BACK)

          def get_json(url: str, retries: int = MAX_RETRIES):
              """Fetch JSON with retry handling for MSRC throttling (HTTP 999)."""
              for i in range(retries):
                  try:
                      r = requests.get(url, headers={"Accept": "application/json"}, timeout=30)
                      if r.status_code == 999:
                          print(f"âš ï¸  MSRC API rate limited (HTTP 999); retrying in {RETRY_SLEEP}sâ€¦ [{i+1}/{retries}]")
                          time.sleep(RETRY_SLEEP)
                          continue
                      r.raise_for_status()
                      # Ensure we can parse JSON (some infra returns HTML on error)
                      try:
                          return r.json()
                      except Exception as e:
                          print(f"âš ï¸  Response was not valid JSON: {e}")
                          time.sleep(10)
                  except Exception as e:
                      print(f"âš ï¸  Request attempt {i+1} failed: {e}")
                      time.sleep(10)
              print("âŒ All retries failed or invalid JSON; returning empty result.")
              return {"value": []}

          def parse_iso(dt_str: str):
              if not dt_str:
                  return None
              try:
                  # Normalize trailing Z to +00:00 for fromisoformat
                  return datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
              except Exception:
                  return None

          data = get_json(URL)
          items = data.get("value", [])
          if isinstance(items, dict) and "value" in items:
              items = items["value"]

          print(f"ðŸ”Ž SUG returned items: {len(items)}")

          rows = []
          for item in items:
              # Pick the best date available on the record
              dt = (parse_iso(item.get("currentReleaseDate"))
                    or parse_iso(item.get("releaseDate"))
                    or parse_iso(item.get("lastModifiedDate")))
              if not dt:
                  continue
              if not (SINCE <= dt <= NOW):
                  continue

              cves = item.get("cveIds") or []
              title = item.get("title") or ""
              for cve in cves:
                  rows.append({
                      "cve": cve,
                      "title": title,
                      "releaseDate": dt.strftime("%Y-%m-%d"),
                      "url": f"https://msrc.microsoft.com/update-guide/vulnerability/{cve}"
                  })

          # Deduplicate by CVE+date (in case the API returns duplicates)
          seen = set()
          unique_rows = []
          for r in rows:
              key = (r["cve"], r["releaseDate"])
              if key in seen:
                  continue
              seen.add(key)
              unique_rows.append(r)

          if not unique_rows:
              print(f"âš ï¸  No valid entries found in window {SINCE.date()}..{NOW.date()}.")
              # Preserve existing data.json by not overwriting it
              if os.path.exists("data.json"):
                  print("â„¹ï¸  Existing data.json preserved.")
              else:
                  # Create an empty file so the site can build cleanly on first run
                  with open("data.json", "w", encoding="utf-8") as f:
                      json.dump([], f, ensure_ascii=False, indent=2)
                  print("â„¹ï¸  Created empty data.json (first run, no results).")
          else:
              unique_rows.sort(key=lambda x: (x["releaseDate"], x["cve"]), reverse=True)
              with open("data.json", "w", encoding="utf-8") as f:
                  json.dump(unique_rows, f, ensure_ascii=False, indent=2)
              print(f"âœ… Wrote {len(unique_rows)} entries to data.json (window {SINCE.date()}..{NOW.date()}).")
          PY

      - name: Commit if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): daily MSRC data.json update"
          file_pattern: data.json
          commit_author: "GitHub Actions <actions@github.com>"
